{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The topic on which news you want to find:\"machine learning\"\n",
      "machine learning\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import bs4 as bs\n",
    "import urllib2\n",
    "from urllib2 import urlopen\n",
    "import datetime\n",
    "#import pandas as pd\n",
    "#import csv\n",
    "#import json\n",
    "from pymongo import MongoClient\n",
    "\n",
    "conn = MongoClient(\"localhost\", 27017)\n",
    "conn = MongoClient()\n",
    "db=conn.tryThree\n",
    "collection = db.cThree\n",
    "\n",
    "\n",
    "\n",
    "arr=[]\n",
    "\n",
    "entry=input('The topic on which news you want to find:')\n",
    "print(entry)\n",
    "\n",
    "\n",
    "\n",
    "#BBC START\n",
    "e=entry.split(\" \")\n",
    "#print(e)\n",
    "fin='+'.join(e)\n",
    "\n",
    "source=['https://www.bbc.co.uk/search?q='+fin+'&sa_f=search-product&scope=#page=15']\n",
    "\n",
    "\n",
    "#print(\"----------BBC NEWS----------\")\n",
    "for n in range(0,1):\n",
    "    page=urllib2.urlopen('https://www.bbc.co.uk/search?q='+fin+'&sa_f=search-product&scope=#page=15').read()\n",
    "    soup=bs.BeautifulSoup(page,'lxml')\n",
    "    for i in soup.find_all('section',{'class':'search-content'}):\n",
    "        for j in i.find_all('ol',{'class':'search-results results'}):\n",
    "            for k in j.find_all('article',{'class':'has_image media-video'}):\n",
    "                #print(\"**********\")\n",
    "                dict={}\n",
    "                for l in k.find_all('div'):\n",
    "                    for n in l.find_all('a'):\n",
    "                        dict['headlines']=n.text\n",
    "                        dict['link']=n.get('href')\n",
    "                        \n",
    "                for m in k.find_all('aside',{'class':'flags top'}):\n",
    "                    for l in m.find_all('dd'):\n",
    "                        f=l.text\n",
    "                        d=str(datetime.datetime.strptime(f,'\\n\\n           %d %b %Y       \\n'))\n",
    "                        dict['date']=d\n",
    "                        \n",
    "                for o in k.find_all('div'):\n",
    "                    for p in o.find_all('p',{'class':'summary medium'}):\n",
    "                        dict['descrip']= p.text\n",
    "                arr.append(dict)\n",
    "                collection.insert_one(dict)\n",
    "            \n",
    "                    \n",
    "            for k in j.find_all('article',{'class':'has_image media-text'}):\n",
    "                #print(\"**********\")\n",
    "                dict={}\n",
    "                for l in k.find_all('div'):\n",
    "                    for n in l.find_all('a'):\n",
    "                        dict['headlines']=n.text\n",
    "                        dict['link']=n.get('href')\n",
    "                        \n",
    "                for m in k.find_all('aside',{'class':'flags top'}):\n",
    "                    for l in m.find_all('dd'):\n",
    "                        f=l.text\n",
    "                        d=str(datetime.datetime.strptime(f,'\\n\\n           %d %b %Y       \\n'))\n",
    "                        dict['date']=d\n",
    "                        \n",
    "                for o in k.find_all('div'):\n",
    "                    for p in o.find_all('p',{'class':'summary medium'}):\n",
    "                        dict['descrip']= p.text\n",
    "                arr.append(dict)\n",
    "                collection.insert_one(dict)\n",
    "            for k in j.find_all('article',{'class':'has_image media-audio'}):\n",
    "                #print(\"**********\")\n",
    "                dict={}\n",
    "                for l in k.find_all('div'):\n",
    "                    for n in l.find_all('a'):\n",
    "                        dict['headlines']=n.text\n",
    "                        dict['link']=n.get('href')\n",
    "                        \n",
    "                for m in k.find_all('aside',{'class':'flags top'}):\n",
    "                    for l in m.find_all('dd'):\n",
    "                        f=l.text\n",
    "                        d=str(datetime.datetime.strptime(f,'\\n\\n           %d %b %Y       \\n'))\n",
    "                        dict['date']=d\n",
    "                        \n",
    "                for o in k.find_all('div'):\n",
    "                    for p in o.find_all('p',{'class':'summary medium'}):\n",
    "                        dict['descrip']= p.text\n",
    "                arr.append(dict)\n",
    "                collection.insert_one(dict)\n",
    "#BBC END\n",
    "                        \n",
    "\n",
    "#BLOOMBERG START\n",
    "\n",
    "e=entry.split(\" \")\n",
    "#print(e)\n",
    "fin='+'.join(e)\n",
    "\n",
    "#print(\"----------BLOOMBERG NEWS-----------\")\n",
    "dateCnt = 0\n",
    "headCnt = 0\n",
    "linksCnt = 0\n",
    "descCnt = 0\n",
    "for d in range(1,30):\n",
    "    page=urllib2.urlopen('https://www.bloomberg.com/search?query='+fin+'&endTime=2018-05-21T06:34:12.628Z&page='+str(d)).read()\n",
    "    soup=bs.BeautifulSoup(page,'lxml')\n",
    "    #print(\"page\"+str(d))\n",
    "    li=[]\n",
    "    count=0\n",
    "    for i in soup.find_all('div',{'class':'container'}):\n",
    "        for j in i.find_all('main',{'id':'content'},{'class':'main-content'}):\n",
    "            for k in j.find_all('section',{'class':'content-stories'}):\n",
    "                for l in k.find_all('div',{'class':'search-result-story__container'}):\n",
    "                    dict={}   \n",
    "                    for o in l.find_all('div',{'class':'search-result-story__metadata'}):\n",
    "                        for p in o.find_all('span',{'class':'metadata-timestamp'}):\n",
    "                            for q in p.find_all('time',{'class':'published-at'}):\n",
    "                                if len(li)<=10:\n",
    "                                    li.append(q.text)\n",
    "                                     \n",
    "                    for m in l.find_all('h1',{'class':'search-result-story__headline'}):\n",
    "                        for n in m.find_all('a'):\n",
    "                            #print(\"*******************\")\n",
    "                            dict['headlines']=n.text\n",
    "                            dict['link']=n.get('href')\n",
    "                            dt2=li[count]\n",
    "                            dt3=datetime.datetime.strptime(dt2,' %b %d, %Y ')\n",
    "                            dict['date']=str(dt3)\n",
    "                            headCnt+=1\n",
    "                            linksCnt+=1\n",
    "                            dateCnt+=1\n",
    "                            count=count+1\n",
    "                            \n",
    "               \n",
    "                    for r in l.find_all('div',{'class':'search-result-story__body'}):\n",
    "                        dict['descrip']=r.text\n",
    "                        descCnt+=1\n",
    "                            \n",
    "                    arr.append(dict)\n",
    "                    collection.insert_one(dict)\n",
    "#bloomberg end   \n",
    "\n",
    "                  \n",
    "#NYTimes Start\n",
    "                    \n",
    "e=entry.split(\" \")\n",
    "#print(e)\n",
    "fin='%20'.join(e)   \n",
    "\n",
    "\n",
    "#print(\"----------NEWYORK TIMES----------\")\n",
    "\n",
    "dateCnt = 0\n",
    "headCnt = 0\n",
    "linksCnt = 0\n",
    "descCnt = 0\n",
    "\n",
    "for z in range(0,1):\n",
    "    page=urllib2.urlopen('https://www.nytimes.com/search?query='+fin+'&sort=best').read()\n",
    "    soup=bs.BeautifulSoup(page,'lxml')\n",
    "    for i in soup.find_all('div',{'id':'app'}):\n",
    "        for n in i.find_all('div',{'class':'Item-item--2UnE8'}):\n",
    "            for o in n.find_all('div',{'Item-wrapper--2ba8L'}):\n",
    "                dict={}\n",
    "                for j in o.find_all('a'):\n",
    "                    link=j.get('href')\n",
    "                    flag=0\n",
    "                    if link.startswith('https'):\n",
    "                        flag=1\n",
    "                    if (link.endswith('html') and flag==0):\n",
    "                        #print(\"**************\")\n",
    "                        dict['link']=link\n",
    "                        linksCnt+=1\n",
    "                    no=link.find('2')\n",
    "                    if(no>=0):\n",
    "                       dt4=link[no:11]\n",
    "                       if dt4=='20':\n",
    "                           da=None\n",
    "                           #print(\" DATE : NIL\")\n",
    "                       else:\n",
    "                           c=str(datetime.datetime.strptime(dt4,'%Y/%m/%d'))\n",
    "                           da=c \n",
    "                        \n",
    "                        \n",
    "                    else:\n",
    "                        da=None\n",
    "                        \n",
    "                    dict['date']=da\n",
    "                    dateCnt+=1\n",
    "                for k in o.find_all('h4',{'class':'Item-headline--3WqlT'}):\n",
    "                    dict['headlines']=k.text\n",
    "                    headCnt+=1\n",
    "                 \n",
    "                for l in o.find_all('p',{'class':'Item-summary--3nKWX'}):\n",
    "                    dict['descrip']=l.text \n",
    "                    descCnt+=1\n",
    "            arr.append(dict)\n",
    "            collection.insert_one(dict)\n",
    "                \n",
    "          \n",
    "                \n",
    "#NYTimes end\n",
    "\n",
    "#TIMESOFINDIA start\n",
    "                        \n",
    "                        \n",
    "e=entry.split(\" \")\n",
    "#print(e)\n",
    "fin='-'.join(e)   \n",
    "\n",
    "\n",
    "\n",
    "#print(\"----------TIMES OF INDIA----------\")\n",
    "\n",
    "for z in range(0,1):\n",
    "    page=urllib2.urlopen('https://timesofindia.indiatimes.com/topic/'+fin+'/news/'+str(z)).read()\n",
    "    soup=bs.BeautifulSoup(page,'lxml')\n",
    "    #print(soup.prettify())\n",
    "    for i in soup.find_all('div',{'class':'content'}):\n",
    "        dict={}\n",
    "        for k in i.find_all('span',{'class':'title'}):\n",
    "            #print(k.prettify)\n",
    "            #print(\"*****************\")\n",
    "            dict['headlines']=k.text\n",
    "            \n",
    "            \n",
    "            \n",
    "        for j in i.find_all('a'):\n",
    "            dict['link']=j.get('href')\n",
    "            for l in j.find_all('span',{'class':'meta'}):\n",
    "                dd=l.text\n",
    "                            \n",
    "                f=dd.find('Z')\n",
    "                if f>=0:\n",
    "                    da1=dd[:10]\n",
    "\n",
    "                else:\n",
    "                    c=str(datetime.datetime.strptime(dd,'%d %b %Y'))\n",
    "                    da1=c\n",
    "                dict['date']=da1                        \n",
    "            for m in j.find_all('p'):\n",
    "                dict['descrip']=m.text\n",
    "        arr.append(dict)\n",
    "        collection.insert_one(dict)    \n",
    "#TIMESOFINDIA End\n",
    "                \n",
    "                \n",
    "#CNBC START\n",
    "                \n",
    "e=entry.split(\" \")\n",
    "#print(e)\n",
    "fin='%20'.join(e)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(\"----------CNBC NEWS----------\")\n",
    "\n",
    "\n",
    "for z in range(0,1):\n",
    "    page=urllib2.urlopen('https://search.cnbc.com/rs/search/view.html?target=all&categories=exclude&partnerId=2000&keywords='+fin).read()\n",
    "    soup=bs.BeautifulSoup(page,'lxml')\n",
    "    for i in soup.find_all('div',{'class':'SearchResultCard'}):\n",
    "        #print(i.prettify())\n",
    "        dict={}\n",
    "        for j in i.find_all('h3',{'class':'title'}):\n",
    "            #print(j.prettify())\n",
    "            for k in j.find_all('a'):\n",
    "                #print(\"*****************\")\n",
    "                dict['headlines']=k.text\n",
    "                linnk=k.get('href')\n",
    "                dict['link']=linnk\n",
    "                lin_k=linnk.split(\"com/\")\n",
    "                linnnk=lin_k[1]\n",
    "                no=linnnk.find('2018')\n",
    "                if no>=0:\n",
    "                    #print(\"DATE : \"+linnnk[:10])\n",
    "                    d=linnnk[:10]\n",
    "                    dt4=linnnk[:10]\n",
    "                    d=str(datetime.datetime.strptime(dt4,'%Y/%m/%d'))\n",
    "                    da2=d\n",
    "                else:\n",
    "                    da2=None\n",
    "                dict['date']=da2\n",
    "                \n",
    "                \n",
    "                        \n",
    "        for l in i.find_all('p',{'class':'description'}):\n",
    "            dict['descrip']=l.text\n",
    "        arr.append(dict) \n",
    "        collection.insert_one(dict)\n",
    "        \n",
    "\n",
    "#CNBC END\n",
    "\n",
    "\n",
    "\n",
    "#finalDict={\"data\":arr}  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
